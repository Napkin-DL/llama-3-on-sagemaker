{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43eaad7f-f315-4f50-938a-fd94f07ca15a",
   "metadata": {},
   "source": [
    "# <B> SageMaker pileline with `MLflow` </B>\n",
    "* Container: codna_pytorch_p310\n",
    "* [Example codes](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-mlflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e8867-4692-4d7c-8009-127c7125489b",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d83680b-5ed0-4e16-b5e5-14bd6b3d3a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b26cf7-b510-4508-b86f-bd18e3e96a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc649e2-9aa5-4e99-9ea4-8cc80af63404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_model_id : MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "bucket : sagemaker-us-west-2-322537213286\n",
      "prefix : sagemaker/llama-3-1-kor-bllossom-8b\n",
      "model_weight_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/model_weight/MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "training_input_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/train/train_dataset.json\n",
      "test_input_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/test/test_dataset.json\n",
      "local_training_input_path : /home/ec2-user/SageMaker/2024/llama-3-on-sagemaker/dataset/train\n",
      "local_test_input_path : /home/ec2-user/SageMaker/2024/llama-3-on-sagemaker/dataset/test\n",
      "tracking_server_arn : arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801\n",
      "experiment_name : llama-3-1-kor-bllossom-8b-240801\n",
      "registered_model : llama-3-1-kor-bllossom-8b\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_model_id : {test_model_id}\")\n",
    "print(f\"bucket : {bucket}\")\n",
    "print(f\"prefix : {prefix}\")\n",
    "print(f\"model_weight_path : {model_weight_path}\")\n",
    "print(f\"training_input_path : {training_input_path}\")\n",
    "print(f\"test_input_path : {test_input_path}\")\n",
    "print(f\"local_training_input_path : {local_training_input_path}\")\n",
    "print(f\"local_test_input_path : {local_test_input_path}\")\n",
    "print(f\"tracking_server_arn : {tracking_server_arn}\")\n",
    "print(f\"experiment_name : {experiment_name}\")\n",
    "print(f\"registered_model : {registered_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d796a-cebe-4283-9364-f05ae7dd744c",
   "metadata": {},
   "source": [
    "## 1. Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba096c0-9ef1-4026-a2c2-a121bedb4708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20230604T222555\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "import argparse\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "from sagemaker.workflow.retry import StepRetryPolicy, StepExceptionTypeEnum, SageMakerJobExceptionTypeEnum, SageMakerJobStepRetryPolicy\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region_name = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27bb4dad-a5fa-46e0-a94e-acf96b800d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(\n",
    "    enable_caching=True,\n",
    "    expire_after=\"T48H\"\n",
    ")\n",
    "\n",
    "retry_policies=[\n",
    "    # retry when resource limit quota gets exceeded\n",
    "    SageMakerJobStepRetryPolicy(\n",
    "        exception_types=[SageMakerJobExceptionTypeEnum.RESOURCE_LIMIT],\n",
    "        expire_after_mins=180,\n",
    "        interval_seconds=60,\n",
    "        backoff_rate=1.0\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a21d7c3-82cb-48b3-8781-f1bbc97dd4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_package_name : MPG-llama-3-1-kor-bllossom-8b\n",
      "serving_instance_type : ml.g5.12xlarge\n"
     ]
    }
   ],
   "source": [
    "## 환경 셋팅\n",
    "model_package_name = \"-\".join([\"MPG\", registered_model])\n",
    "\n",
    "deployexecution_instance = \"ml.m5.xlarge\"\n",
    "endpoint_name = f'endpoint--{model_package_name}-{int(time.time())}'\n",
    "proc_prefix_path = \"/opt/ml/processing\"\n",
    "\n",
    "serving_instance_type = \"ml.g5.12xlarge\"\n",
    "# serving_instance_type = \"ml.g5.4xlarge\"\n",
    "# serving_instance_type = \"ml.g5.xlarge\"\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "max_run = 24*60*60\n",
    "\n",
    "print(f\"model_package_name : {model_package_name}\")\n",
    "print(f\"serving_instance_type : {serving_instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29befb82-d312-4e10-89d5-bfd0894af1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.workflow.pipeline_context.PipelineSession at 0x7fb3330433a0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if serving_instance_type =='local_gpu':\n",
    "    pipeline_session = LocalPipelineSession()\n",
    "    pipeline_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "else:\n",
    "    pipeline_session = PipelineSession()\n",
    "pipeline_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13051611-fc73-4dec-a935-b7d301c5a429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n",
      "ml.g5.12xlarge and # of GPU 4 is set\n",
      "The current time is 2024-08-02-03-32-23\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    "  version=\"2.0.2\",\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "\n",
    "if serving_instance_type == \"ml.p4d.24xlarge\":\n",
    "    num_GPUSs = 8\n",
    "elif serving_instance_type == \"ml.g5.12xlarge\":\n",
    "    num_GPUSs = 4\n",
    "elif serving_instance_type == \"ml.g5.4xlarge\":\n",
    "    num_GPUSs = 1    \n",
    "else:\n",
    "    num_GPUSs = 1\n",
    "    \n",
    "print(f\"{serving_instance_type} and # of GPU {num_GPUSs} is set\")\n",
    "\n",
    "# sagemaker config\n",
    "currentTime = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "print(\"The current time is\", currentTime)\n",
    "\n",
    "health_check_timeout = 600 # 20 minutes\n",
    "model_name = f\"llama3-model-{currentTime}\"\n",
    "\n",
    "import time\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",       # Path to the model in the container\n",
    "    \"SM_NUM_GPUS\": f\"{num_GPUSs}\",        # Number of GPU used per replica\n",
    "    \"MAX_INPUT_LENGTH\": \"2048\",           # Max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\",           # Max length of the generation (including input text)\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    \"MESSAGES_API_ENABLED\": \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    model_data=f\"{compressed_model_path}/model.tar.gz\", # path to s3 bucket with model, we are not using a compressed model\n",
    "    image_uri=llm_image,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    env=config,\n",
    ")\n",
    "\n",
    "from sagemaker import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    models=[llm_model],\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "register_model_step_args = pipeline_model.register(\n",
    "    content_types=[\"*\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g5.2xlarge\", \"ml.g5.12xlarge\"],\n",
    "    # transform_instances=[\"ml.g5.2xlarge\", \"ml.g5.12xlarge\"],\n",
    "    model_package_group_name=model_package_name,\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    ## “Approved”, “Rejected”, or “PendingManualApproval” (default: “PendingManualApproval”).\n",
    ")\n",
    "\n",
    "registration_process = ModelStep(\n",
    "   name=\"ModelRegistrationProcess\",\n",
    "   step_args=register_model_step_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9ea8773-bf5b-446b-92ad-73d04bc8e1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "== Deploy Step ==\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import FrameworkProcessor\n",
    "\n",
    "deploy_processor = FrameworkProcessor(\n",
    "    estimator_cls=PyTorch,\n",
    "    framework_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    image_uri=None,\n",
    "    role=role,\n",
    "    instance_type=deployexecution_instance,\n",
    "    instance_count=instance_count,\n",
    "    base_job_name=\"deploy\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "step_deploy_args = deploy_processor.run(\n",
    "    code=\"deploy.py\",\n",
    "    source_dir=\"src/deploy/\",\n",
    "    arguments=[\n",
    "        \"--prefix_deploy\", proc_prefix_path, \\\n",
    "        \"--region\", region_name, \\\n",
    "        \"--instance_type\", serving_instance_type, \\\n",
    "        \"--depoly_instance_type\", serving_instance_type, \\\n",
    "        \"--model_package_group_name\", model_package_name, \\\n",
    "        \"--endpoint_name\", endpoint_name, \\\n",
    "        \"--execution_role\", role, \\\n",
    "    ],\n",
    "    job_name=\"deploy\",\n",
    ")\n",
    "\n",
    "deploy_process = ProcessingStep(\n",
    "    name=\"DeployProcess\", ## Processing job이름\n",
    "    step_args=step_deploy_args,\n",
    "    depends_on=[register_process],\n",
    "    cache_config=cache_config,\n",
    "    retry_policies=retry_policies\n",
    ")\n",
    "\n",
    "print (\"  \\n== Deploy Step ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1d1206f-e074-4714-a582-c9a7f9d8011d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.processing:Uploaded src/deploy/ to s3://sagemaker-us-west-2-322537213286/MPG-llama-3-1-kor-bllossom-8b/code/98519768ad2a31f79f83576f9448c86a/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-west-2-322537213286/MPG-llama-3-1-kor-bllossom-8b/code/be2cbd35f419f747dca87a487b64f344/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.processing:Uploaded src/deploy/ to s3://sagemaker-us-west-2-322537213286/MPG-llama-3-1-kor-bllossom-8b/code/98519768ad2a31f79f83576f9448c86a/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-west-2-322537213286/MPG-llama-3-1-kor-bllossom-8b/code/be2cbd35f419f747dca87a487b64f344/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/MPG-llama-3-1-kor-bllossom-8b',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/MPG-llama-3-1-kor-bllossom-8b/execution/q6mqjupfcooh',\n",
       " 'PipelineExecutionDisplayName': 'execution-1722569547718',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2024, 8, 2, 3, 32, 27, 658000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 8, 2, 3, 32, 27, 658000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::322537213286:assumed-role/AmazonSageMaker-ExecutionRole-20230604T222555/SageMaker',\n",
       "   'PrincipalId': 'AROAUWGFXJVTJPN7KR2AE:SageMaker'}},\n",
       " 'LastModifiedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::322537213286:assumed-role/AmazonSageMaker-ExecutionRole-20230604T222555/SageMaker',\n",
       "   'PrincipalId': 'AROAUWGFXJVTJPN7KR2AE:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': '64664e6c-9b94-48ee-8f4b-a690d512356c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '64664e6c-9b94-48ee-8f4b-a690d512356c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '755',\n",
       "   'date': 'Fri, 02 Aug 2024 03:32:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=model_package_name,\n",
    "    steps=[registration_process, deploy_process],\n",
    "    #steps=[self.preprocessing_process, self.training_process],\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role) ## Submit the pipeline definition to the SageMaker Pipelines service \n",
    "execution = pipeline.start()\n",
    "desc = execution.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e2576-a995-41db-8f5f-2c381fa7bfe8",
   "metadata": {},
   "source": [
    "### model package approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d262ed5-36ab-42a3-a3c6-a3c24106dae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MPG-llama-3-1-kor-bllossom-8b'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01894be3-ecec-4c51-ada8-e20e20853f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackageArn : arn:aws:sagemaker:us-west-2:322537213286:model-package/MPG-llama-3-1-kor-bllossom-8b/2\n",
      "ModelApprovalStatus : Approved\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "sm_model_package = sm_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_name,\n",
    "    SortBy=\"CreationTime\",\n",
    "    SortOrder=\"Descending\"\n",
    ")\n",
    "\n",
    "model_package_arn=sm_model_package['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "ModelApprovalStatus=sm_model_package['ModelPackageSummaryList'][0]['ModelApprovalStatus']\n",
    "\n",
    "print(f\"ModelPackageArn : {model_package_arn}\")\n",
    "print(f\"ModelApprovalStatus : {ModelApprovalStatus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2406249-206f-42d6-927a-a3cb5c61d180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 승인 \n",
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : model_package_arn,\n",
    "    \"ModelApprovalStatus\" : \"Approved\"\n",
    "}\n",
    "model_package_update_response = sm_client.update_model_package(**model_package_update_input_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9088b8f8-f818-4b5d-8bba-34f86709afac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineArn:  arn:aws:sagemaker:us-west-2:322537213286:pipeline/MPG-llama-3-1-kor-bllossom-8b\n",
      "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/MPG-llama-3-1-kor-bllossom-8b', 'PipelineExecutionArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/MPG-llama-3-1-kor-bllossom-8b/execution/q6mqjupfcooh', 'PipelineExecutionDisplayName': 'execution-1722569547718', 'PipelineExecutionStatus': 'Executing', 'PipelineExperimentConfig': {'ExperimentName': 'mpg-llama-3-1-kor-bllossom-8b', 'TrialName': 'q6mqjupfcooh'}, 'CreationTime': datetime.datetime(2024, 8, 2, 3, 32, 27, 658000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2024, 8, 2, 3, 32, 27, 658000, tzinfo=tzlocal()), 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::322537213286:assumed-role/AmazonSageMaker-ExecutionRole-20230604T222555/SageMaker', 'PrincipalId': 'AROAUWGFXJVTJPN7KR2AE:SageMaker'}}, 'LastModifiedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::322537213286:assumed-role/AmazonSageMaker-ExecutionRole-20230604T222555/SageMaker', 'PrincipalId': 'AROAUWGFXJVTJPN7KR2AE:SageMaker'}}, 'ResponseMetadata': {'RequestId': 'd0a6afd7-4f7a-4abc-a01d-203fbead3763', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd0a6afd7-4f7a-4abc-a01d-203fbead3763', 'content-type': 'application/x-amz-json-1.1', 'content-length': '860', 'date': 'Fri, 02 Aug 2024 03:37:16 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print (\"PipelineArn: \", desc[\"PipelineArn\"])\n",
    "print (execution.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a4de5e8-362e-4e88-a9c2-0e069843d5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_prompt = '서울의 유명한 관광 코스를 만들어줄래?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b44e43ed-2ccb-491e-9b86-f197440e07c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ],\n",
    "    \"model\": \"meta-llama-3-fine-tuned\",\n",
    "    \"parameters\": {\"max_tokens\":256,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"max_tokens\": 1024,\n",
    "                \"stop\": [\"<|eot_id|>\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "267ea7fd-c99b-4316-a42a-a85007c06fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 1.406 second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OfFile(서울의 유명한 관광 코스)\\n\\n1. 경복궁\\n- 경복궁 약도: 경복궁 - 인사동 - 고̉스_CO.commands/Cheese\\\\d ilişkinonuangsyou.txt\\n 경복궁은 조선 시대의 대표적인 궁궐로, 한국의 역사와 문화를 느낄 수 있는 곳입니다. 경복궁의 건물과 정원, 그리고 고궁박물관을 방문할 수 있습니다.\\n\\n2.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Set up the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "s = time.perf_counter()\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    # InferenceComponentName = inference_component_name_llama3b,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(request_body)\n",
    ")\n",
    "# Get the response from the endpoint\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "parsed_data = json.loads(result)\n",
    "answer = parsed_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2d8b4-5f03-43da-ab0f-af76fe59d91d",
   "metadata": {},
   "source": [
    "### Resource 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d5d780d-0c0e-4aec-8b2a-d41bbbf6150b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "379fd6c4-ab53-445d-9771-6cb6eb7b8942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: llama3-model-2024-08-02-03-32-23\n",
      "An error occurred (ValidationException) when calling the DescribeEndpointConfig operation: Could not find endpoint configuration \"endpoint--MPG-llama-3-1-kor-bllossom-8b-1722564633\".\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5fd4b68e-89be-4f2a-89fd-2e4c693ab0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: endpoint--MPG-llama-3-1-kor-bllossom-8b-1722564633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint: [b magenta]endpoint--MPG-llama-3-1-kor-bllossom-8b-1722564633 ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: endpoint--MPG-llama-3-1-kor-bllossom-8b-1722564633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4fd27-5964-48c5-be97-c9130be50058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
