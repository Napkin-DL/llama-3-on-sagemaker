{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune Llama 3 with PyTorch FSDP and Q-Lora\n",
    "- https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_model_id : MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "bucket : sagemaker-us-west-2-322537213286\n",
      "prefix : sagemaker/llama-3-1-kor-bllossom-8b\n",
      "model_weight_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/model_weight/MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "training_input_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/train/train_dataset.json\n",
      "test_input_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/test/test_dataset.json\n",
      "local_training_input_path : /home/ec2-user/SageMaker/2024/llama-3-on-sagemaker/dataset/train\n",
      "local_test_input_path : /home/ec2-user/SageMaker/2024/llama-3-on-sagemaker/dataset/test\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_model_id : {test_model_id}\")\n",
    "print(f\"bucket : {bucket}\")\n",
    "print(f\"prefix : {prefix}\")\n",
    "print(f\"model_weight_path : {model_weight_path}\")\n",
    "print(f\"training_input_path : {training_input_path}\")\n",
    "print(f\"test_input_path : {test_input_path}\")\n",
    "print(f\"local_training_input_path : {local_training_input_path}\")\n",
    "print(f\"local_test_input_path : {local_test_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.226.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## training script 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Llama-3 on Amazon SageMaker\n",
    "\n",
    "In addition to our `deepspeed_parameters` we need to define the `training_hyperparameters` for our training script. The `training_hyperparameters` are passed to our `training_script` as CLI arguments with `--key value`. \n",
    "\n",
    "If you want to better understand which batch_size and `deepspeed_config` can work which hardware setup you can check out the [Results & Experiments](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) we ran.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src/configs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir src/configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/configs/llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/configs/llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_name_or_path: \"/opt/ml/input/data/model_weight\"        # Hugging Face model id\n",
    "train_dataset_path: \"/opt/ml/input/data/training\"                      # path to dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test\"\n",
    "max_seq_length: 256\n",
    "# training parameters\n",
    "output_dir: \"/opt/ml/checkpoints\"     # Temporary output directory for model checkpoints\n",
    "# report_to: \"wandb\"                  # report metrics to tensorboard\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 2                    # number of training epochs\n",
    "per_device_train_batch_size: 16         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: steps                  #epoch                   # save checkpoint every epoch\n",
    "save_steps: 1000\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: false                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide the ARN of the tracking server that you want to track your training job with\n",
    "tracking_server_arn = '<your tracking server arn here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "# training_hyperparameters={\n",
    "#     \"wb_token\" : \"<your wandb token>\"\n",
    "# }\n",
    "training_hyperparameters={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance_type = 'ml.p4d.24xlarge' # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "# instance_type = 'ml.g5.12xlarge'\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "# instance_type = 'ml.p5.48xlarge'\n",
    "instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "max_run = 24*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model_weight_path = f\"{Path.cwd()}/model_weight/{test_model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/train/train_dataset.json',\n",
       " 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/gemini_result_kospi_0517/test/test_dataset.json',\n",
       " 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/model_weight/MLP-KTLim/llama-3-Korean-Bllossom-8B')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    training = f\"file://{local_training_input_path}\"\n",
    "    test = f\"file://{local_test_input_path}\"\n",
    "    model_weight = f\"file://{local_model_weight_path}\"\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    training = training_input_path\n",
    "    test = test_input_path\n",
    "    model_weight = model_weight_path\n",
    "\n",
    "training, test, model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-llama-3-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# distribution={ \"pytorchddp\": { \"enabled\": True } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
    "distribution={\n",
    "    \"torch_distributed\": {\n",
    "        \"enabled\": True,\n",
    "        # \"NCCL_DEBUG\":\"INFO\"\n",
    "        # \"mpi\": \"-verbose -x NCCL_DEBUG=INFO\"\n",
    "    }\n",
    "}  # torchrun, activates SMDDP AllGather\n",
    "# distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
    "\n",
    "environment={\n",
    "    \"NCCL_DEBUG\" : \"INFO\", \n",
    "    \"SM_LOG_LEVEL\": \"10\",\n",
    "    \"MLFLOW_TRACKING_ARN\": tracking_server_arn\n",
    "}\n",
    "\n",
    "training_hyperparameters[\"config\"] = \"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\"\n",
    "    \n",
    "estimator = PyTorch(\n",
    "                    entry_point='run_fsdp_qlora.py',\n",
    "                    source_dir=f'{Path.cwd()}/src',\n",
    "                    role=role,\n",
    "                    # image_uri=image_uri,\n",
    "                    framework_version='2.2.0',\n",
    "                    py_version='py310',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    # metric_definitions=metric_definitions,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters={\n",
    "                      **training_hyperparameters,\n",
    "                    },   # the hyperparameter used for running the training job\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    enable_remote_debug=True,\n",
    "                    # keep_alive_period_in_seconds=1200,\n",
    "                    # input_mode='FastFile'\n",
    "                    # max_wait=max_run,\n",
    "                    # use_spot_instances=True,\n",
    "                    # subnets=['subnet-090e278f3622051c4'],\n",
    "                    # security_group_ids=['sg-05baa06337a188842'],\n",
    "                    max_retry_attempts=30,\n",
    "                    environment = environment,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created our `HuggingFace` estimator including the `ds_launcher.py` as `entry_point` and defined our `deepspeed` config and `training_script` in the `deepspeed_parameters`, which we merged with our `training_hyperparameters`. We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf src/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama-3-ml-g5-48xlarge-1-0731-08031722413018\n"
     ]
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'llama-3-{i_type}-{instance_count}-{current_time}'\n",
    "\n",
    "## additional setting for mlflow\n",
    "estimator._hyperparameters[\"model_uri\"] = f's3://{bucket}/{prefix}/checkpoint/{test_model_id}/{job_name}'\n",
    "estimator.environment[\"MLFLOW_EXPERIMENT_NAME\"] = prefix.split(\"/\")[-1]\n",
    "estimator.environment[\"MLFLOW_RUN_NAME\"] = job_name\n",
    "\n",
    "\n",
    "if instance_type =='local_gpu':\n",
    "    estimator.checkpoint_s3_uri = None\n",
    "else:\n",
    "    estimator.checkpoint_s3_uri = estimator._hyperparameters[\"model_uri\"] \n",
    "    \n",
    "    \n",
    "estimator.fit(\n",
    "    # inputs={'training': s3_data_path, 'model_weight': model_weight}, \n",
    "    inputs={\n",
    "        'training': training,\n",
    "        'test': test,\n",
    "        'model_weight' : model_weight\n",
    "    }, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 08:03:39 Starting - Starting the training job...\n",
      "2024-07-31 08:03:40 Pending - Training job waiting for capacity...................................................\n",
      "2024-07-31 08:12:37 Pending - Preparing the instances for training......\n",
      "2024-07-31 08:13:37 Downloading - Downloading input data.....................\n",
      "2024-07-31 08:16:58 Downloading - Downloading the training image...\n",
      "2024-07-31 08:17:34 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:36,863 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:36,926 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:36,938 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:36,940 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:36,940 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:17:38,520 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.40.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.0/138.0 kB 5.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.28.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate>=0.4.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft>=0.10.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.19.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl>=0.8.6 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes>=0.43.1 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb>=0.16.6 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn>=2.5.7 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.6.3.tar.gz (2.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting mlflow-skinny==2.13.2 (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-2.13.2-py3-none-any.whl.metadata (30 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-mlflow==0.1.0 (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.40.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.40.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 6.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers==4.40.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (2.2.1)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints<1 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (6.11.0)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-api<3,>=1.0.0 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-sdk<3,>=1.0.0 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (2024.1)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.34 in /opt/conda/lib/python3.10/site-packages (from sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.34.114)\u001b[0m\n",
      "\u001b[34mCollecting mlflow>=2.8 (from sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.15.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 3)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 3)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate>=0.4.1->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 3)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate>=0.4.1->-r requirements.txt (line 3)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->-r requirements.txt (line 5)) (16.1.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb>=0.16.6->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb>=0.16.6->-r requirements.txt (line 8)) (4.1.0)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb>=0.16.6->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.11.0-py2.py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb>=0.16.6->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb>=0.16.6->-r requirements.txt (line 8)) (71.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn>=2.5.7->-r requirements.txt (line 9)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.35.0,>=1.34.114 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.34.114)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.16.6->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.3.2-py3-none-any.whl.metadata (5.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.13.2->-r requirements.txt (line 10)) (3.19.2)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting mlflow>=2.8 (from sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.14.3-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting Flask<4 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=15.0.0 (from datasets>=2.19.0->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser<2 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.13.1)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy<3,>=1.4.0 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.1.4)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn<23 (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.0.0->mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<3,>=1.0.0->mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate>=0.4.1->-r requirements.txt (line 3)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate>=0.4.1->-r requirements.txt (line 3)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.0.3)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.1.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.6.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.13.2->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (4.52.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (10.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r requirements.txt (line 11)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 103.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 41.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-2.13.2-py3-none-any.whl (5.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 102.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl (296 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.4/296.4 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 56.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 36.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.5/137.5 MB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 99.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 81.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.3-py3-none-any.whl (417 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.3/417.3 kB 46.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.14.3-py3-none-any.whl (25.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/25.8 MB 62.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 8.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.5/109.5 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.0/138.0 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 48.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.5/776.5 kB 53.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 303.6/303.6 kB 41.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.2/44.2 kB 7.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 88.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.4/103.4 kB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 33.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.3.2-py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.0/233.0 kB 29.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 19.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 36.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 9.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.4/84.4 kB 15.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 86.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 39.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 9.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202.9/202.9 kB 29.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 14.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187219571 sha256=8991eedb5038a1ee6fc9904f99c12b40213d66753ed91e261a43d085f5aeab8f\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: aniso8601, xxhash, wrapt, sqlparse, sqlalchemy, smmap, shtab, setproctitle, sentry-sdk, safetensors, regex, querystring-parser, pyarrow-hotfix, pyarrow, multidict, Mako, itsdangerous, gunicorn, graphql-core, frozenlist, entrypoints, docstring-parser, docker-pycreds, cachetools, blinker, async-timeout, aiohappyeyeballs, yarl, huggingface-hub, graphql-relay, gitdb, Flask, deprecated, alembic, aiosignal, tyro, tokenizers, opentelemetry-api, graphene, gitpython, flash-attn, bitsandbytes, aiohttp, accelerate, wandb, transformers, opentelemetry-semantic-conventions, peft, opentelemetry-sdk, datasets, trl, mlflow-skinny, mlflow, evaluate, sagemaker-mlflow\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pyarrow\u001b[0m\n",
      "\u001b[34mFound existing installation: pyarrow 16.1.0\u001b[0m\n",
      "\u001b[34mUninstalling pyarrow-16.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pyarrow-16.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.0.4\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.0.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.0.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mtransformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.6.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Flask-3.0.3 Mako-1.3.5 accelerate-0.28.0 aiohappyeyeballs-2.3.2 aiohttp-3.10.0 aiosignal-1.3.1 alembic-1.13.2 aniso8601-9.0.1 async-timeout-4.0.3 bitsandbytes-0.43.3 blinker-1.8.2 cachetools-5.4.0 datasets-2.20.0 deprecated-1.2.14 docker-pycreds-0.4.0 docstring-parser-0.16 entrypoints-0.4 evaluate-0.4.2 flash-attn-2.6.3 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 huggingface-hub-0.24.3 itsdangerous-2.2.0 mlflow-2.14.3 mlflow-skinny-2.13.2 multidict-6.0.5 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 peft-0.12.0 pyarrow-15.0.2 pyarrow-hotfix-0.6 querystring-parser-1.2.4 regex-2024.7.24 safetensors-0.4.3 sagemaker-mlflow-0.1.0 sentry-sdk-2.11.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 sqlalchemy-2.0.31 sqlparse-0.5.1 tokenizers-0.19.1 transformers-4.40.1 trl-0.9.6 tyro-0.8.5 wandb-0.17.5 wrapt-1.16.0 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.0 -> 24.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,735 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,735 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,819 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,898 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,912 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,976 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-31 08:18:23,990 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model_weight\": \"/opt/ml/input/data/model_weight\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\n",
      "        \"model_uri\": \"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model_weight\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_fsdp_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_fsdp_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_fsdp_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model_weight\",\"test\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_fsdp_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL_WEIGHT=/opt/ml/input/data/model_weight\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_URI=s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 8 run_fsdp_qlora.py --config /opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml --model_uri s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\u001b[0m\n",
      "\u001b[34m[2024-07-31 08:18:25,271] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-07-31 08:18:25,271] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-07-31 08:18:25,271] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-07-31 08:18:25,271] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '0', 'RANK': '0', 'GROUP_RANK': '0', 'ROLE_RANK': '0', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/0/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '1', 'RANK': '1', 'GROUP_RANK': '0', 'ROLE_RANK': '1', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/1/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '6', 'RANK': '6', 'GROUP_RANK': '0', 'ROLE_RANK': '6', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/6/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '5', 'RANK': '5', 'GROUP_RANK': '0', 'ROLE_RANK': '5', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/5/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '7', 'RANK': '7', 'GROUP_RANK': '0', 'ROLE_RANK': '7', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/7/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '3', 'RANK': '3', 'GROUP_RANK': '0', 'ROLE_RANK': '3', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/3/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '4', 'RANK': '4', 'GROUP_RANK': '0', 'ROLE_RANK': '4', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/4/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mos.environ : environ({'SM_INPUT_DIR': '/opt/ml/input', 'NCCL_DEBUG': 'INFO', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'run_fsdp_qlora.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.g5.48xlarge', 'EFA_VERSION': '1.31.0', 'MLFLOW_RUN_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'MASTER_ADDR': '127.0.0.1', 'SM_MODULE_NAME': 'run_fsdp_qlora', 'HOSTNAME': 'ip-10-0-206-26.us-west-2.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'NVTE_FRAMEWORK': 'pytorch', 'HOME': '/root', 'MASTER_PORT': '29500', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_HP_CONFIG': '/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'NCCL_PROTO': 'simple', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-570aa06f29077cab3e9b10d71ddaa13672159cbe8483db956a5f717f8c7a76f3-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '192', 'CUDA_VERSION': '12.1.0', 'TORCH_CUDA_ARCH_LIST': '5.2;7.0+PTX;7.5;8.0;8.6;9.0', 'SM_NUM_GPUS': '8', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'MLFLOW_TRACKING_ARN': 'arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801', 'GDRCOPY_VERSION': '2.4.1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama-3-ml-g5-48xlarge-1-0731-08031722413018\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/llama-3-ml-g5-48xlarge-1-0731-08031722413018/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}', 'MLFLOW_EXPERIMENT_NAME': 'llama-3-1-kor-bllossom-8b', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'REQUESTS_CA_BUNDLE': '/etc/ssl/certs/ca-certificates.crt', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}', 'SM_CHANNEL_MODEL_WEIGHT': '/opt/ml/input/data/model_weight', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"config\":\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"model_uri\":\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.9.2.26', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"model_weight\",\"test\",\"training\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'FI_PROVIDER': 'efa', 'AWS_REGION': 'us-west-2', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_DLC_TORCH_VERSION': '2.2.0', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/opt/conda', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-west-2', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.21.5', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--config\",\"/opt/ml/code/configs/llama_3_8b_fsdp_qlora.yaml\",\"--model_uri\",\"s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MODEL_URI': 's3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '2', 'RANK': '2', 'GROUP_RANK': '0', 'ROLE_RANK': '2', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_5n1h5nk2/none_s70ktmcb/attempt_0/2/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_CPU_RAM_EFFICIENT_LOADING': '1', 'ACCELERATE_USE_IPEX': 'false', 'ACCELERATE_MIXED_PRECISION': 'bf16', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_OFFLOAD_PARAMS': 'true', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'false', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_USE_ORIG_PARAMS': 'false'})\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 230 examples [00:00, 60527.66 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 230 examples [00:00, 107140.15 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34malgo-1:190:190 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:190:190 [0] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:190:190 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:190:190 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:190:190 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO Bootstrap : Using eth0:10.0.206.26<0>\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO comm 0x560bc69382b0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO comm 0x563fa4c91310 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO comm 0x55d454d303a0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 190 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO comm 0x56347ac2acd0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO comm 0x55bd649784e0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 180 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO comm 0x55984112b980 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 170 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO comm 0x55a9d30c9510 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO comm 0x5638d3f064e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 160 commId 0x7723b7edd6bb73e3 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Channel 00 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Channel 01 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Channel 00 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Channel 01 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Channel 00 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Channel 00 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Channel 01 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Channel 01 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:192:914 [2] NCCL INFO comm 0x55bd649784e0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 180 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:190:911 [0] NCCL INFO comm 0x5638d3f064e0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 160 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:196:916 [6] NCCL INFO comm 0x56347ac2acd0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:194:917 [4] NCCL INFO comm 0x563fa4c91310 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:195:912 [5] NCCL INFO comm 0x560bc69382b0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:191:918 [1] NCCL INFO comm 0x55984112b980 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 170 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:193:915 [3] NCCL INFO comm 0x55d454d303a0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 190 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:197:913 [7] NCCL INFO comm 0x55a9d30c9510 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x7723b7edd6bb73e3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s><s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s><s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 광전자의 주가 보고서는 어떤가요? [/INST] **증권 보고서 요약**\u001b[0m\n",
      "\u001b[34m**날짜:** 2024년 5월 17일\u001b[0m\n",
      "\u001b[34m**종목 정보:**\u001b[0m\n",
      "\u001b[34m* 종목명: 광전자\u001b[0m\n",
      "\u001b[34m* 종목 코드: 017900\u001b[0m\n",
      "\u001b[34m**등락률:**\u001b[0m\n",
      "\u001b[34m해당 날짜에 광전자의 등락률 데이터는 제공되지 않습니다.\u001b[0m\n",
      "\u001b[34m**매수/매도 신호:**\u001b[0m\n",
      "\u001b[34m* 매수 신호:\n",
      "    * 전기전자 지수 (정배열)\n",
      "    * 제조업 지수 (정배열) </s><s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s><s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s><s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34m<s>[INST] 2024-05-17에 HMM의 주가 보고서는 어떤가요? [/INST] 증권 보고서 없음 </s>\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:17,  2.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:22,  3.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:22,  3.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:22,  3.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:23,  3.86s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:21,  3.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:23,  3.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:23,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:15,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:07<00:18,  3.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:09<00:12,  3.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:11<00:15,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:12<00:09,  3.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:14<00:11,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:15<00:11,  3.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:15<00:06,  3.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:18<00:03,  3.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:18<00:07,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:19<00:07,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:20<00:00,  2.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:20<00:00,  2.91s/it]\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:22<00:03,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.53s/it]\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.17s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.55s/it]\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.55s/it]\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34msft_config.output_dir : /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 131 examples [00:00, 1986.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 131 examples [00:00, 3818.30 examples/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 41,943,040 || all params: 8,072,220,672 || trainable%: 0.5196\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 1/2 [00:24<00:24, 24.29s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/17 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 2/17 [00:02<00:17,  1.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 3/17 [00:04<00:23,  1.65s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 4/17 [00:07<00:24,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 5/17 [00:09<00:24,  2.05s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 6/17 [00:11<00:23,  2.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 7/17 [00:13<00:22,  2.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 8/17 [00:16<00:20,  2.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 9/17 [00:18<00:18,  2.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 10/17 [00:20<00:16,  2.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 11/17 [00:23<00:13,  2.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 12/17 [00:25<00:11,  2.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 13/17 [00:27<00:09,  2.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 14/17 [00:30<00:06,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 15/17 [00:32<00:04,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 16/17 [00:34<00:02,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 17/17 [00:37<00:00,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.8678505420684814, 'eval_runtime': 40.7146, 'eval_samples_per_second': 3.218, 'eval_steps_per_second': 0.418, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 1/2 [01:05<00:24, 24.29s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 17/17 [00:37<00:00,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [01:26<00:00, 46.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/17 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 2/17 [00:02<00:17,  1.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 3/17 [00:04<00:23,  1.65s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 4/17 [00:06<00:24,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 5/17 [00:09<00:24,  2.05s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 6/17 [00:11<00:23,  2.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 7/17 [00:13<00:21,  2.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 8/17 [00:16<00:20,  2.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 9/17 [00:18<00:18,  2.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 10/17 [00:20<00:15,  2.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 11/17 [00:23<00:13,  2.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 12/17 [00:25<00:11,  2.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 13/17 [00:27<00:09,  2.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 14/17 [00:30<00:06,  2.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 15/17 [00:32<00:04,  2.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 16/17 [00:34<00:02,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 17/17 [00:37<00:00,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.627639055252075, 'eval_runtime': 40.6276, 'eval_samples_per_second': 3.224, 'eval_steps_per_second': 0.418, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [02:06<00:00, 46.30s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 17/17 [00:37<00:00,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 126.6437, 'train_samples_per_second': 2.069, 'train_steps_per_second': 0.016, 'train_loss': 2.7961509227752686, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [02:06<00:00, 46.30s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [02:06<00:00, 63.32s/it]\u001b[0m\n",
      "\u001b[34malgo-1:193:926 [3] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34malgo-1:196:924 [6] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34malgo-1:191:922 [1] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34malgo-1:192:919 [2] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34malgo-1:195:923 [5] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34malgo-1:194:925 [4] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34malgo-1:197:921 [7] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34malgo-1:193:193 [3] NCCL INFO comm 0x55d454d303a0 rank 3 nranks 8 cudaDev 3 busId 190 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [6] NCCL INFO comm 0x56347ac2acd0 rank 6 nranks 8 cudaDev 6 busId 1c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [1] NCCL INFO comm 0x55984112b980 rank 1 nranks 8 cudaDev 1 busId 170 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:195:195 [5] NCCL INFO comm 0x560bc69382b0 rank 5 nranks 8 cudaDev 5 busId 1b0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:192:192 [2] NCCL INFO comm 0x55bd649784e0 rank 2 nranks 8 cudaDev 2 busId 180 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:197:197 [7] NCCL INFO comm 0x55a9d30c9510 rank 7 nranks 8 cudaDev 7 busId 1d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [4] NCCL INFO comm 0x563fa4c91310 rank 4 nranks 8 cudaDev 4 busId 1a0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34mRegistered model 'llama-3-1-kor-bllossom-8b' already exists. Creating a new version of this model...\u001b[0m\n",
      "\u001b[34m2024/07/31 08:21:34 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: llama-3-1-kor-bllossom-8b, version 2\u001b[0m\n",
      "\u001b[34mCreated version '2' of model 'llama-3-1-kor-bllossom-8b'.\u001b[0m\n",
      "\u001b[34malgo-1:190:920 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\n",
      "2024-07-31 08:21:45 Uploading - Uploading generated training model\u001b[34malgo-1:190:190 [0] NCCL INFO comm 0x5638d3f064e0 rank 0 nranks 8 cudaDev 0 busId 160 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m2024-07-31 08:21:40,734 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:21:40,734 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-31 08:21:40,734 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-07-31 08:21:53 Completed - Training job completed\n",
      "Training seconds: 495\n",
      "Billable seconds: 495\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT 모델 추론 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_result = sagemaker_session.describe_training_job(job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0731-08031722413018'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_s3uri = train_result['CheckpointConfig']['S3Uri']\n",
    "output_dir = './checkpoints/20240731'\n",
    "checkpoint_s3uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync $checkpoint_s3uri $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48d75901b6d44438cb03cedc88e5f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "peft_model_id = output_dir\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_weight_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "merged_save_dir = \"merged_model\"\n",
    "peft_model.save_pretrained(merged_save_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_weight_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(merged_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울의 유명한 관광 코스를 만들어줄래? 서울의 다양한 명소를 소개해줄게! 😊\n",
      "\n",
      "서울은 역사와 문화, 자연과 현대가 공존하는 도시로, 다양한 명소를 통해 다양한 경험을 할 수 있어. 여기 몇 가지 추천할게!\n",
      "\n",
      "1. **경복궁**: 조선 왕조의 궁궐로, 한국의 역사와 문화를 체험할 수 있는 곳. 특히, 왕의 생활을 체험할 수 있는 '궁중 체험 프로그램'이 인기가 많아. 🏯\n",
      "\n",
      "2. **남산서울타워**: 서울의 전경을 한눈에 볼 수 있는 곳. 특히, 저녁 시간에는 서울의 야경이 아름답게 보인다. 🌆\n",
      "\n",
      "3. **홍대**: 젊음과 예술의 중심지. 다양한 카페, 레스토랑, 갤러리, 클럽 등이 몰려 있어, 밤에 방문하면 더욱 분위기가 좋다. 🎨\n",
      "\n",
      "4. **청계천**: 서울의 중심부를 흐르는 천. 다양한 공연과 이벤트가 열리는 곳으로, 특히, 야간에 방문하면 아름다운 조명이 켜져 있어. 💧\n",
      "\n",
      "5\n",
      "CPU times: user 18min 59s, sys: 1min 21s, total: 20min 21s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_new_tokens = 250\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"서울의 유명한 관광 코스를 만들어줄래?\", return_tensors=\"pt\"\n",
    ").input_ids  \n",
    "\n",
    "outputs = peft_model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'merged_save_dir'\n"
     ]
    }
   ],
   "source": [
    "%store merged_save_dir\n",
    "%store checkpoint_s3uri\n",
    "%store tracking_server_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
