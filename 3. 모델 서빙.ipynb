{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference Component 모델 배포 및 추론\n",
    "\n",
    "아래 모델 배포는 아래의 인스턴스에서 테스트 완료 되었습니다.\n",
    "- [Pricing Link](https://aws.amazon.com/sagemaker/pricing/)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 구성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 상위 폴더의 Python 경로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking_server_arn : arn:aws:sagemaker:us-west-2:322537213286:mlflow-tracking-server/mlflow-tracking-240801\n",
      "tracking_server_arn : llama-3-1-kor-bllossom-8b-240801\n",
      "compressed_model_path : s3://sagemaker-us-west-2-322537213286/sagemaker/llama-3-1-kor-bllossom-8b/checkpoint/MLP-KTLim/llama-3-Korean-Bllossom-8B/llama-3-ml-g5-48xlarge-1-0801-13441722519862/compressed_model\n"
     ]
    }
   ],
   "source": [
    "print(f\"tracking_server_arn : {tracking_server_arn}\")\n",
    "print(f\"tracking_server_arn : {experiment_name}\")\n",
    "print(f\"compressed_model_path : {compressed_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 이미지 가져오기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20230604T222555\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_client = sess.sagemaker_client\n",
    "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    "  version=\"2.0.2\",\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint 생성\n",
    "- EndpointConfiguration 생성\n",
    "- Endpoint 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 배포할 인스턴스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.g5.12xlarge and # of GPU 4 is set\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "# # instance_type = \"ml.g5.4xlarge\"\n",
    "# instance_type = \"ml.g5.xlarge\"\n",
    "\n",
    "\n",
    "if instance_type == \"ml.p4d.24xlarge\":\n",
    "    num_GPUSs = 8\n",
    "elif instance_type == \"ml.g5.12xlarge\":\n",
    "    num_GPUSs = 4\n",
    "elif instance_type == \"ml.g5.4xlarge\":\n",
    "    num_GPUSs = 1    \n",
    "else:\n",
    "    num_GPUSs = 1\n",
    "    \n",
    "print(f\"{instance_type} and # of GPU {num_GPUSs} is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint config name 및 설정 값 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time is 2024-08-02-01-40-35\n",
      "Endpoint config name: llama3-endpoint-config-2024-08-02-01-40-35\n",
      "Initial instance count: 1\n",
      "Max instance count: 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "currentTime = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "print(\"The current time is\", currentTime)\n",
    "\n",
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"llama3-endpoint-config-{currentTime}\" \n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "model_data_download_timeout_in_seconds = 600\n",
    "container_startup_health_check_timeout_in_seconds = 600\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 1\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Configuration 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epc_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: llama3-endpoint-2024-08-02-01-40-35\n",
      "Creating endpoint: llama3-endpoint-2024-08-02-01-40-35\n",
      "-----!CPU times: user 31.1 ms, sys: 8.66 ms, total: 39.8 ms\n",
      "Wall time: 3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'llama3-endpoint-2024-08-02-01-40-35',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:322537213286:endpoint/llama3-endpoint-2024-08-02-01-40-35',\n",
       " 'EndpointConfigName': 'llama3-endpoint-config-2024-08-02-01-40-35',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1,\n",
       "   'ManagedInstanceScaling': {'Status': 'ENABLED',\n",
       "    'MinInstanceCount': 1,\n",
       "    'MaxInstanceCount': 1},\n",
       "   'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2024, 8, 2, 1, 40, 35, 661000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 8, 2, 1, 43, 25, 349000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'f193d65c-b3ce-43fc-8c34-d58257e9355e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f193d65c-b3ce-43fc-8c34-d58257e9355e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '574',\n",
       "   'date': 'Fri, 02 Aug 2024 01:43:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"llama3-endpoint-{currentTime}\"\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "\n",
    "ep_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "# print(ep_response)\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model 정의\n",
    "- 추론 이미지 기술 \n",
    "- 모델 아티펙트 경로 기술 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-01 23:18:32 14120580802 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $compressed_model_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189723/1685951317.py:5: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.13.2/model-registry.html#migrating-from-stages\n",
      "  res_mlflow = client.get_latest_versions(registered_model)\n"
     ]
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Case-sensitive name\n",
    "client = MlflowClient()\n",
    "res_mlflow = client.get_latest_versions(registered_model)\n",
    "registered_model_version = res_mlflow[0].version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "\n",
    "health_check_timeout = 600 # 20 minutes\n",
    "model_name = f\"llama3-model-{currentTime}\"\n",
    "\n",
    "import time\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",       # Path to the model in the container\n",
    "    \"SM_NUM_GPUS\": f\"{num_GPUSs}\",        # Number of GPU used per replica\n",
    "    \"MAX_INPUT_LENGTH\": \"2048\",           # Max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\",           # Max length of the generation (including input text)\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    \"MESSAGES_API_ENABLED\": \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    model_data=f\"{compressed_model_path}/model.tar.gz\", # path to s3 bucket with model, we are not using a compressed model\n",
    "    image_uri=llm_image,\n",
    "    env=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference component 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_component_name_llama3b:  llama3b-IC-2024-08-02-01-40-35\n"
     ]
    }
   ],
   "source": [
    "# Deploy model to Amazon SageMaker Inference Component\n",
    "inference_component_name_llama3b = f\"llama3b-IC-{currentTime}\"\n",
    "print(\"inference_component_name_llama3b: \", inference_component_name_llama3b)\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "try:\n",
    "    sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_llama3b)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ic_response = sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_llama3b,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": num_GPUSs,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferenceComponent: llama3b-IC-2024-08-02-01-40-35\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "# Wait for IC to come InService\n",
    "print(f\"InferenceComponent: {inference_component_name_llama3b}\")\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceComponentName': 'llama3b-IC-2024-08-02-01-40-35',\n",
       " 'InferenceComponentArn': 'arn:aws:sagemaker:us-west-2:322537213286:inference-component/llama3b-IC-2024-08-02-01-40-35',\n",
       " 'EndpointName': 'llama3-endpoint-2024-08-02-01-40-35',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:322537213286:endpoint/llama3-endpoint-2024-08-02-01-40-35',\n",
       " 'VariantName': 'AllTraffic',\n",
       " 'Specification': {'ModelName': 'llama3-model-2024-08-02-01-40-35',\n",
       "  'ComputeResourceRequirements': {'NumberOfCpuCoresRequired': 1.0,\n",
       "   'NumberOfAcceleratorDevicesRequired': 4.0,\n",
       "   'MinMemoryRequiredInMb': 1024}},\n",
       " 'RuntimeConfig': {'DesiredCopyCount': 1, 'CurrentCopyCount': 1},\n",
       " 'CreationTime': datetime.datetime(2024, 8, 2, 1, 43, 38, 892000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 8, 2, 1, 49, 0, 597000, tzinfo=tzlocal()),\n",
       " 'InferenceComponentStatus': 'InService',\n",
       " 'ResponseMetadata': {'RequestId': '35553664-9fa8-4d83-9309-c1fb7a8c4867',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '35553664-9fa8-4d83-9309-c1fb7a8c4867',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '724',\n",
       "   'date': 'Fri, 02 Aug 2024 01:49:09 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_prompt = '서울의 유명한 관광 코스를 만들어줄래?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ],\n",
    "    \"model\": \"meta-llama-3-fine-tuned\",\n",
    "    \"parameters\": {\"max_tokens\":256,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"max_tokens\": 512,\n",
    "                \"stop\": [\"<|eot_id|>\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 5.048 second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'서울은 긴 발표가 필요할 만큼 가득 차 있지만, 몇 가지 대표적인 관광 코스를 몇 가지 추천해 줄 수 있습니다.\\n\\n1. **경복궁과 인사동 길**\\n   - **경복궁**: 조선 왕조의 깊은 역사를 느낄 수 있는 곳으로, 대성전, 광화문, 창덕궁 등이 있다.\\n   - **인사동 길**: 전통 차림, 가게와cafe가 많'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Set up the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "s = time.perf_counter()\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName = inference_component_name_llama3b,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(request_body)\n",
    ")\n",
    "# Get the response from the endpoint\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "parsed_data = json.loads(result)\n",
    "answer = parsed_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 리소스 삭제\n",
    "- 인퍼런스 컴포넌트 삭제\n",
    "- 세이지 메이커 모델 삭제\n",
    "- 엔드포인트 삭제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting inference components: [b magenta]llama3b-IC-2024-08-02-01-40-35 ✅\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Deleting inference components: [b magenta]{inference_component_name_llama3b} ✅\")\n",
    "    # Delete inference component\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: llama3-model-2024-08-02-01-40-35\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint: [b magenta]llama3-endpoint-2024-08-02-01-40-35 ✅\n",
      "------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
